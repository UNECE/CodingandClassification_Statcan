{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following ECOICOP data used for this example is open-source, provided by Statistics Poland\n",
    "\n",
    "# Link: https://statswiki.unece.org/download/attachments/256969394/Stats%20Poland%20ECOICOP%20data%20translated%20to%20English%20and%20French.xlsx?version=1&modificationDate=1570023568166&api=v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define file locations ###\n",
    "model_dir = \"Z:\\\\Team_Folders\\\\Evans\\\\python_scripts\\\\HLG_MOS\\\\Poland_FastText\\\\Data\\\\French\\\\\"\n",
    "output_dir = (\"Z:\\\\Team_Folders\\\\Evans\\\\python_scripts\\\\HLG_MOS\\\\Poland_FastText\\\\Data\\\\French\\\\\")\n",
    "\n",
    "# model confidence threshold to apply\n",
    "threshold = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = fasttext.load_model(model_dir+\"model.bin\")\n",
    "test_data = os.path.join(os.getenv(\"DATADIR\",\"\"),model_dir+\"test.txt\")\n",
    "overall_acc = loaded_model.test(test_data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__024',), array([0.29182354]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(\"this is an example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>code_text</th>\n",
       "      <th>code</th>\n",
       "      <th>formatted</th>\n",
       "      <th>pred</th>\n",
       "      <th>score</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Jus de clementine 100%</td>\n",
       "      <td>Jus de fruits et de légumes</td>\n",
       "      <td>027</td>\n",
       "      <td>__label__027 - Jus de clementine 100%</td>\n",
       "      <td>027</td>\n",
       "      <td>1.00001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7UP - Boisson gazeuse citron-lime, 2 x 2 ...</td>\n",
       "      <td>Boissons rafraîchissantes</td>\n",
       "      <td>012</td>\n",
       "      <td>__label__012 7UP - Boisson gazeuse citron-lime...</td>\n",
       "      <td>012</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bigos du chasseur cuisine polonaise</td>\n",
       "      <td>Plats cuisinés</td>\n",
       "      <td>037</td>\n",
       "      <td>__label__037 Bigos du chasseur cuisine polonaise</td>\n",
       "      <td>037</td>\n",
       "      <td>0.972294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mimolle a la Francaise affinee en tranches</td>\n",
       "      <td>Fromage et caillé</td>\n",
       "      <td>020</td>\n",
       "      <td>__label__020 Mimolle a la Francaise affinee en...</td>\n",
       "      <td>020</td>\n",
       "      <td>0.992604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pringles - Frites au fromage et a l&amp;#39;oignon</td>\n",
       "      <td>chips</td>\n",
       "      <td>051</td>\n",
       "      <td>__label__051 Pringles - Frites au fromage et a...</td>\n",
       "      <td>051</td>\n",
       "      <td>0.610564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text  \\\n",
       "0                        - Jus de clementine 100%   \n",
       "1    7UP - Boisson gazeuse citron-lime, 2 x 2 ...   \n",
       "2             Bigos du chasseur cuisine polonaise   \n",
       "3      Mimolle a la Francaise affinee en tranches   \n",
       "4  Pringles - Frites au fromage et a l&#39;oignon   \n",
       "\n",
       "                     code_text code  \\\n",
       "0  Jus de fruits et de légumes  027   \n",
       "1    Boissons rafraîchissantes  012   \n",
       "2               Plats cuisinés  037   \n",
       "3            Fromage et caillé  020   \n",
       "4                        chips  051   \n",
       "\n",
       "                                           formatted pred     score match  \n",
       "0              __label__027 - Jus de clementine 100%  027   1.00001     1  \n",
       "1  __label__012 7UP - Boisson gazeuse citron-lime...  012  0.998325     1  \n",
       "2   __label__037 Bigos du chasseur cuisine polonaise  037  0.972294     1  \n",
       "3  __label__020 Mimolle a la Francaise affinee en...  020  0.992604     1  \n",
       "4  __label__051 Pringles - Frites au fromage et a...  051  0.610564     1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test data for manual verification and to assess predictions\n",
    "df = pd.read_csv(model_dir+\"test.csv\", encoding='UTF-8', dtype=str)\n",
    "df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "df[\"pred\"] =\"\"\n",
    "df[\"score\"] = \"\"\n",
    "df[\"match\"] = \"\"\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    pred = loaded_model.predict(row[\"formatted\"])\n",
    "    row[\"score\"] = pred[1][0]\n",
    "    row[\"pred\"] = pred[0][0]\n",
    "    row[\"pred\"] = row[\"pred\"].replace(\"__label__\",\"\")\n",
    "    row[\"match\"] = 1 if row[\"code\"] ==  row[\"pred\"] else 0\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 96.73\n",
      "Threshold Applied: 0.95\n",
      "Accuracy: 99.45\n",
      "Coding Rate: 90.06\n"
     ]
    }
   ],
   "source": [
    "# basic metrics for accuracy \n",
    "count = 0\n",
    "matches = 0\n",
    "for i,row in df.iterrows():\n",
    "    if row[\"score\"] <= threshold:\n",
    "        continue\n",
    "    matches += int(row[\"match\"])\n",
    "    count +=1\n",
    "\n",
    "accuracy = round(float(matches/count)*100, 2)\n",
    "codingrate = round(float(count/df.shape[0])*100, 2)\n",
    "overall_acc = round(loaded_model.test(test_data)[1]*100, 2)\n",
    "\n",
    "print(\"Overall Accuracy:\",overall_acc)\n",
    "print(\"Threshold Applied:\", threshold)\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"Coding Rate:\",codingrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate Model Performance ##\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weighted Average</td>\n",
       "      <td>96.7</td>\n",
       "      <td>96.8</td>\n",
       "      <td>96.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macro Average</td>\n",
       "      <td>95.6</td>\n",
       "      <td>96.1</td>\n",
       "      <td>95.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type  F1_score  Precision  Recall\n",
       "0  Weighted Average      96.7       96.8    96.7\n",
       "1     Macro Average      95.6       96.1    95.4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix column types\n",
    "df[\"pred\"] = df[\"pred\"].astype(str)\n",
    "df[\"code\"] = df[\"code\"].astype(str)\n",
    "\n",
    "# convert df to list\n",
    "list_actual = df[\"code\"].tolist()\n",
    "list_predicted = df[\"pred\"].tolist()\n",
    "\n",
    "# evaluation metrics\n",
    "weighted_f1 = round(f1_score(list_actual, list_predicted, average = \"weighted\")*100, 2)\n",
    "macro_f1 = round(f1_score(list_actual, list_predicted, average = \"macro\")*100, 2)\n",
    "weighted_precision = round(precision_score(list_actual, list_predicted, average = \"weighted\")*100, 2)\n",
    "macro_precision = round(precision_score(list_actual, list_predicted, average = \"macro\")*100, 2)\n",
    "weighted_recall = round(recall_score(list_actual, list_predicted, average = \"weighted\")*100, 2)\n",
    "macro_recall = round(recall_score(list_actual, list_predicted, average = \"macro\")*100, 2)\n",
    "\n",
    "# create a df to show data\n",
    "metrics = {'Type': ['Weighted Average', 'Macro Average'],\n",
    "              'F1_score': [weighted_f1, macro_f1],\n",
    "            'Precision': [weighted_precision, macro_precision],\n",
    "             'Recall': [weighted_recall, macro_recall]}\n",
    "df_metrics = DataFrame(metrics, columns = ['Type','F1_score', 'Precision', 'Recall'])\n",
    "\n",
    "df_metrics['F1_score'] = df_metrics.apply(lambda row : (round(row[\"F1_score\"], 1)),axis = 1)\n",
    "df_metrics['Precision'] = df_metrics.apply(lambda row : (round(row[\"Precision\"], 1)),axis = 1)\n",
    "df_metrics['Recall'] = df_metrics.apply(lambda row : (round(row[\"Recall\"], 1)),axis = 1)\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a report with model evaluation metrics\n",
    "report = open(model_dir+\"MODEL_METRICS.txt\",\"w\")\n",
    "lines = [\"Overall Accuracy: \"+str(overall_acc)+\"\\n\",  \n",
    "        \"Threshold Applied: \"+str(threshold)+\"\\n\", \n",
    "        \"Accuracy: \"+str(accuracy)+\"\\n\", \n",
    "        \"Coding Rate:\"+str(codingrate)+\"\\n\"+\"\\n\", \n",
    "         \n",
    "         \n",
    "        'Type: '+str('Weighted Average')+\", \"+str('Macro Average')+\"\\n\",\n",
    "        'F1_score: '+str(weighted_f1)+\", \"+str(macro_f1)+\"\\n\",         \n",
    "        'Precision: '+str(weighted_precision)+\", \"+str(macro_precision)+\"\\n\",      \n",
    "        'Recall: '+str(weighted_recall)+\", \"+str(macro_recall)+\"\\n\",      \n",
    "        ]\n",
    "report.writelines(lines) \n",
    "report.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "\n",
    "# create a report - stackoverflow 39662398\n",
    "report = []\n",
    "\n",
    "def classification_report_csv(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split()\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(output_dir+'test_classification_report.csv', index = False)\n",
    "\n",
    "report = classification_report(list_actual, list_predicted)\n",
    "classification_report_csv(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
